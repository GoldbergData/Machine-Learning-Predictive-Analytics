{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network & Classification: \n",
    "The objective is to detect the security breach by predicting suspicious access using an RNN model and the provided Logfile data.\n",
    "\n",
    "Logfile data includes login information like LogID, Timestamp, Method, Path, Status Code, Source, Remote Address, User Agent etc. The last indicator in each row denotes breach(1) and no breach(0) which is the target variable.\n",
    "\n",
    "The expectation is that you will use the keras package to solve this problem (https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing: \n",
    "This data set is a bit messy, so the preprocessing portion is largely a tutorial to make sure students have data ready for keras. \n",
    "\n",
    "a) Import the following libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as k\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We will read the code in slightly differently than before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We then need to convert to a `numpy.ndarray` type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Check the shape of the data set - it should be (26773, 2). Spend some time looking at the data. \n",
    "\n",
    "e) Store all rows and the 0th index as the feature data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Store all rows and index 1 as the target variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) In the next step, we will clean up the predictors. This includes removing features that are not valuable, such as timestamp and source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) We next will tokenize our data, which just means vectorizing our text. Given the data we will tokenize every character (thus char_level = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Need to pad our data as each observation has a different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Create your train set to be 75% of the data and your test set to be 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_processed, Y, test_size=.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model 1 - RNN: \n",
    "The first model will be a pretty minimal RNN with only an embedding layer, LSTM layer, and Dense layer. The next model we will add a few more layers.\n",
    "\n",
    "a) Start by creating an instance of a Sequential model: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) From there, add an Embedding layer: https://keras.io/layers/embeddings/\n",
    "\n",
    "Params:\n",
    "- input_dim = num_words (the variable we created above)\n",
    "- output_dim = 32\n",
    "- input_length = max_log_length (we also created this above) \n",
    "- Keep all other variables as the defaults (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Embedding(input_dim=num_words, output_dim=32, input_length=max_log_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add an LSTM layer https://keras.io/layers/recurrent/#lstm\n",
    "\n",
    "Params:\n",
    "- units = 64\n",
    "- recurrent_dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(LSTM(units=64, recurrent_dropout=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Finally, we will add a Dense layer: https://keras.io/layers/core/#dense \n",
    "\n",
    "Params:\n",
    "- units = 1 (this will be our output)\n",
    "- activation = relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Dense(units=1, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Compile model using the .compile() method: https://keras.io/models/model/\n",
    "\n",
    "Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Use the `.fit()` method to fit the model on the train data. Use `validation_split=0.25`, `epochs=3` `batch_size=128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "model1_fit = model.fit(X_train, Y_train, validation_split=.25, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Use the `.evaluate()` method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Model 2 - RNN + Dropout Layers + New Activation Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add a few new layers to our RNN and switch the activation function. You will be creating a new model here, so make sure to call it something different than the model from Part 2.\n",
    "\n",
    "a) This RNN needs to have the following layers (add in this order):\n",
    "\n",
    "- Embedding Layer (use same params as before)\n",
    "- Dropout Layer (https://keras.io/layers/core/#dropout - use a value of 0.5 for now\n",
    "- LSTM Layer (use same params as before)\n",
    "- Dropout Layer - use a value of 0.5 \n",
    "- Dense Layer - (switch to a sigmoid activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-088da185a26e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodelt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodelt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_log_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodelt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodelt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "k.clear_session()\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, 32, input_length=max_log_length))\n",
    "model2.add(LSTM(64, recurrent_dropout=0.5))\n",
    "model2.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b45f9ce9bd57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_1_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model2_fit = model.fit(X_train, Y_train, validation_split=.25, epochs=3, batch_size=128)\n",
    "model2.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Recurrent Neural Net Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) RNN Requirements: \n",
    "- Use 5 or more layers\n",
    "- Add a layer that was not utilized in Model 1 or Model 2 (Note: This could be a new Dense layer or an additional LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(num_words, 32, input_length=max_log_length))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(64,recurrent_dropout=0.5,return_sequences= True))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(64, recurrent_dropout=0.5))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model3.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "model3.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1310px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
