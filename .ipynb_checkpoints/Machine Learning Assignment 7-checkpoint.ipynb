{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#1.a. Importing libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b reading the code\n",
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.c We then need to convert to a numpy.ndarray type:\n",
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.d.Check the shape of the data set\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.e. Store all rows and the 0th index as the feature data: \n",
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.f. Store all rows and index 1 as the target variable: \n",
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.g. In the next step, we will clean up the predictors. This includes removing features that are not valuable, such as timestamp and source. \n",
    "for index, item in enumerate(X):\n",
    "# Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.h We next will tokenize our data\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i. Need to pad our data as each observation has a different length\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train, Y_test = train_test_split(X_processed,Y,test_size=.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Model 1 - RNN: The first model will be a pretty minimal RNN with only an embedding layer, LSTM layer, and Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Using the relu activation function for the output layer\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,32,input_length=max_log_length))\n",
    "model.add(LSTM(64,recurrent_dropout=0.5))\n",
    "model.add(Dense(1,activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train, epochs=3, batch_size=128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 15s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.566360255444552, 0.6411711979631812]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#running the same model but with a sigmoid activation function in the output layer\n",
    "K.clear_session()\n",
    "modelt = Sequential()\n",
    "modelt.add(Embedding(num_words,32,input_length=max_log_length))\n",
    "modelt.add(LSTM(64,recurrent_dropout=0.5))\n",
    "modelt.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "modelt.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "modelt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 197s 13ms/step - loss: 0.5889 - acc: 0.6809 - val_loss: 0.3546 - val_acc: 0.8705\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 197s 13ms/step - loss: 0.3202 - acc: 0.8833 - val_loss: 0.2101 - val_acc: 0.9629\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 220s 15ms/step - loss: 0.2194 - acc: 0.9399 - val_loss: 0.1318 - val_acc: 0.9735\n"
     ]
    }
   ],
   "source": [
    "history = modelt.fit(X_train,Y_train, epochs=3, batch_size=128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 15s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12534750444179582, 0.9740065731751516]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelt.evaluate(X_test,Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Model 2 - RNN + Dropout Layers + New Activation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words,32,input_length=max_log_length))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(LSTM(64,recurrent_dropout=0.5))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 221s 15ms/step - loss: 0.6090 - acc: 0.6560 - val_loss: 0.3629 - val_acc: 0.8950\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 213s 14ms/step - loss: 0.3606 - acc: 0.8659 - val_loss: 0.2040 - val_acc: 0.9610\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 196s 13ms/step - loss: 0.2977 - acc: 0.9061 - val_loss: 0.1908 - val_acc: 0.9540\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train,Y_train, epochs=3, batch_size=128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 15s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19024268955588874, 0.9514490589833381]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test,Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 4. Build Your Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024, 64)          24832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 59,937\n",
      "Trainable params: 59,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(num_words,32,input_length=max_log_length))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(64,recurrent_dropout=0.5,return_sequences= True))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(64,recurrent_dropout=0.5))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 659s 44ms/step - loss: 0.4811 - acc: 0.7621 - val_loss: 0.2556 - val_acc: 0.8952\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 630s 42ms/step - loss: 0.2775 - acc: 0.8999 - val_loss: 0.3020 - val_acc: 0.9108\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 599s 40ms/step - loss: 0.2091 - acc: 0.9396 - val_loss: 0.1131 - val_acc: 0.9709\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(X_train,Y_train, epochs=3, batch_size=128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 44s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11744119359159741, 0.9687780102830094]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test,Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Explain the difference between the relu activation function and the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLu function is simply y(x)=max(0,x). It is not differentiable at 0 and may result in exploding gradients. The function and its derivative are both monotonic. ReLu does not suffer from the vanishing gradients problem. The ReLU function is non linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n",
    "The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. If the input is negative it will convert it to zero and the neuron does not get activated. This means that at a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n",
    "\n",
    "The sigmoid function is of the form y(x)=1/(1+e^-x) and exists between (0 to 1). It Therefore, it is especially used for models where we have to predict the probability as an output.The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points. The function is monotonic but function’s derivative is not. The biggest con of this function is that it suffers from the vanishing gradients problem.\n",
    "\n",
    "The Sigmoid works better as a binary classifier than ReLu as because of the steep gradient in the central portion of the function, it pushes outputs towards the two extreme ends. ReLu works better in the hidden layers as it is less computationally expensive than sigmoid because it involves simpler mathematical operations, and also does not lead to vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. In regards to question 5, which of these activation functions performed the best (they were used in Model 1 & Model 2) ? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to question 5, sigmoid performs better. Running the same model in Question 2 with sigmoid and relu for activation function in the output layer, we see that sigmoid does a much better job of classifying the output as 1 or 0. This is because it has a very steep gradient in the central part of the function (between x values of -2 to 2). Which means, any small changes in the values of X in that region will cause values of Y to change significantly. This has a tendency to bring the Y values to either end of the curve and makes it better as a classifier. If we were using these activation functions in hidden layers, then relu would have been a better choice, as it does not suffer from the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Explain how dropout works (you can look at the keras code) for (a) training, and (b) test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)For training - During the train phase, Dropout is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 20%) each weight update cycle. This helps prevent overfitting. Dropout is mainly applied during the train phase.  \n",
    "\n",
    "(b)For testing - Dropout is only used during the training of a model and is not used when evaluating the skill of the model. Keras. Keras disables the dropout during the test phase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Explain why problems such as this are better modeled with RNNs than CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem given has data which is temporally related. The convolution operation in a CNN allows a network to share parameters across time, but is shallow. The output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input. The idea of parameter sharing manifests in the application of the same convolution kernel at each time step.\n",
    "\n",
    "Recurrent networks share parameters in a diﬀerent way. Each member of the output is a function of the previous members of the output. Each member of the output is produced using the same update rule applied to the previous outputs. This recurrent formulation results in the sharing of parameters through a very deep computational graph. \n",
    "\n",
    "Parameter sharing makes it possible to extend and apply the model to examples of diﬀerent forms (diﬀerent lengths, here) and generalize across them. If we had separate parameters for each value of the time index, we could not generalize to sequence lengths not seen during training, nor share statistical strength across diﬀerent sequence lengths and across diﬀerent positions in time. \n",
    "\n",
    "It is dues to this property of parameter sharing that makes it better to model problems such as this with RNNs rather than CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Explain what RNN problem is solved using LSTM and briefly describe how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM solves the vanishing (and exploding gradient problem of RNNs). In a multi-layer network, gradients for deeper layers are calculated as products of many gradients (of activation functions). When those gradients are small or zero, it will easily vanish. (On the other hand, when they’re bigger than 1, it will possibly explode.) So it becomes very hard to calculate and update. \n",
    "\n",
    "In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.\n",
    "\n",
    "The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to 1.0), then the gradient does not vanish. Since the forget gate activation is never >1.0, the gradient can't explode either.\n",
    "So that's why LSTM is so good at learning long range dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
